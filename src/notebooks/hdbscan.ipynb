{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN), is a clustering algorithm that can be applied to the AIA171 dataset obtained from the Solar Dynamic Observatory. The dataset consists of images, which are transformed into 1D power spectra to serve as the feature vectors. HDBSCAN enables the identification of clusters within the dataset by considering the density and connectivity of the data points. By applying HDBSCAN to the AIA171 dataset, we can uncover patterns, groupings, and anomalies in the solar observations, facilitating a deeper understanding of the underlying solar dynamics and potential insights into solar phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import glob\n",
    "import PIL.Image as Image\n",
    "from sklearn.manifold import TSNE\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting a 2D image into its 1D Power Spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting a 2D grayscaled image into its 1D power spectrum can be done as such:\n",
    "1. Apply the 2D Fast Fourier Transform (FFT) to the image. This is done using the FFT method in the numpy library.\n",
    "2. Once the FFT is applied, obtain the magnitude spectrum by taking the absolute value of the complex-valued result obtained from the FFT.\n",
    "3. To convert the 2D power spectrum into 1D, average the magnitudes along concentric circles centered at the image's origin (zero frequency). This is done by calculating the average magnitude for each radius, starting from the center and moving outwards.\n",
    "4. The resulting radial profile represents the 1D power spectrum, with the x-axis representing the radius and the y-axis representing the average magnitude or power at each radius.\n",
    "\n",
    "The cell below is the method that follows the steps above to calulate the 1D power specturm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates 1d Power Spectrum\n",
    "def power_spectrum_1d(image):\n",
    "\n",
    "    # Get pixel count\n",
    "    pixel_count = image.shape[0]\n",
    "\n",
    "    # Convert into fourier transform\n",
    "    fourier_image = np.fft.fftn(image)\n",
    "    fourier_amplitudes = np.abs(fourier_image)\n",
    "\n",
    "    #Calculate 1D power spectrum\n",
    "    k_frequencies = np.fft.fftfreq(pixel_count) * pixel_count\n",
    "    k_frequencies2D = np.meshgrid(k_frequencies, k_frequencies)\n",
    "    k_norm = np.sqrt(k_frequencies2D[0] ** 2 + k_frequencies2D[1] ** 2)\n",
    "    k_bins = np.arange(0.5, pixel_count // 2 + 1, 1.)\n",
    "    k_vals = 0.5 * (k_bins[1:] + k_bins[:-1])\n",
    "    a_bins, _, _ = stats.binned_statistic(k_norm.flatten(),\n",
    "                                        (fourier_amplitudes ** 2).flatten(),\n",
    "                                        statistic = \"mean\", bins = k_bins)\n",
    "    a_bins *= np.pi * (k_bins[1:] ** 2 - k_bins[:-1] ** 2)\n",
    "    \n",
    "    return a_bins\n",
    "\n",
    "# Calculates Wasserstein distance of two images\n",
    "def wasserstein(x, y): \n",
    "    return stats.wasserstein_distance(np.arange(len(x)), np.arange(len(y)), x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Dataset\n",
    "This cell creates two vectors: One to store the original image represented as a numpy array, and another to store the 1D power spectrum of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all images in AIA171_Miniset_BW and represent them as 1D power specturm numpy arrays\n",
    "images = []\n",
    "x = []\n",
    "image_paths = glob.glob('./../../AIA171_Miniset_BW/**/*.jpg', recursive = True)\n",
    "for image_path in image_paths:\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    image = np.array(image)\n",
    "    image = image.astype(float) / 255\n",
    "    pow_spect = power_spectrum_1d(image)\n",
    "    if pow_spect.any():\n",
    "        images.append(image)\n",
    "        x.append(pow_spect)\n",
    "        \n",
    "images = np.array(images)\n",
    "x = np.array(x)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction using t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a dimensionality reduction technique commonly used for visualizing high-dimensional data in a lower-dimensional space. It aims to capture the underlying structure and relationships within the data by preserving the pairwise similarities between data points.\n",
    "\n",
    "The cell below employs t-SNE using the scikit-learn method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_x = TSNE().fit_transform(x)\n",
    "\n",
    "x_max = np.max(embedded_x, axis=0)\n",
    "x_min = np.min(embedded_x, axis=0)\n",
    "embedded_x = (embedded_x - x_min) / (x_max - x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unknown algorithm type memory specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m hdbscan\u001b[39m.\u001b[39mHDBSCAN(min_cluster_size\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, min_samples\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, metric \u001b[39m=\u001b[39m wasserstein, algorithm\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmemory\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(x)\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39mscatter(embedded_x[:,\u001b[39m0\u001b[39m], embedded_x[:,\u001b[39m1\u001b[39m], c \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlabels_)\n\u001b[0;32m      5\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mHDBSCAN Clustering with EMD Metric\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\starf\\miniconda3\\envs\\sdo_research\\lib\\site-packages\\hdbscan\\hdbscan_.py:1205\u001b[0m, in \u001b[0;36mHDBSCAN.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1195\u001b[0m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mprediction_data\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   1196\u001b[0m kwargs\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metric_kwargs)\n\u001b[0;32m   1198\u001b[0m (\n\u001b[0;32m   1199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels_,\n\u001b[0;32m   1200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobabilities_,\n\u001b[0;32m   1201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcluster_persistence_,\n\u001b[0;32m   1202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condensed_tree,\n\u001b[0;32m   1203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_single_linkage_tree,\n\u001b[0;32m   1204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_min_spanning_tree,\n\u001b[1;32m-> 1205\u001b[0m ) \u001b[39m=\u001b[39m hdbscan(clean_data, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1207\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mprecomputed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_all_finite:\n\u001b[0;32m   1208\u001b[0m     \u001b[39m# remap indices to align with original data in the case of non-finite entries.\u001b[39;00m\n\u001b[0;32m   1209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condensed_tree \u001b[39m=\u001b[39m remap_condensed_tree(\n\u001b[0;32m   1210\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condensed_tree, internal_to_raw, outliers\n\u001b[0;32m   1211\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\starf\\miniconda3\\envs\\sdo_research\\lib\\site-packages\\hdbscan\\hdbscan_.py:812\u001b[0m, in \u001b[0;36mhdbscan\u001b[1;34m(X, min_cluster_size, min_samples, alpha, cluster_selection_epsilon, max_cluster_size, metric, p, leaf_size, algorithm, memory, approx_min_span_tree, gen_min_span_tree, core_dist_n_jobs, cluster_selection_method, allow_single_cluster, match_reference_implementation, **kwargs)\u001b[0m\n\u001b[0;32m    797\u001b[0m         (single_linkage_tree, result_min_span_tree) \u001b[39m=\u001b[39m memory\u001b[39m.\u001b[39mcache(\n\u001b[0;32m    798\u001b[0m             _hdbscan_boruvka_balltree\n\u001b[0;32m    799\u001b[0m         )(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    810\u001b[0m         )\n\u001b[0;32m    811\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 812\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown algorithm type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m specified\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m algorithm)\n\u001b[0;32m    813\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m issparse(X) \u001b[39mor\u001b[39;00m metric \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m FAST_METRICS:\n\u001b[0;32m    816\u001b[0m         \u001b[39m# We can't do much with sparse matrices ...\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unknown algorithm type memory specified"
     ]
    }
   ],
   "source": [
    "model = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=1, metric = wasserstein)\n",
    "model.fit(x)\n",
    "\n",
    "plt.scatter(embedded_x[:,0], embedded_x[:,1], c = model.labels_)\n",
    "plt.title('HDBSCAN Clustering with EMD Metric')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdo_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
